---
title: "Car Price Regression Analysis"
author: "Jinminli-1008264361"
date: "2024-12-03"
output:
  pdf_document: default
  html_document: default
  fontsize: 12pt
geometry: a4paper
header-includes:
  - \usepackage{titling}  # To format the title page
  - \usepackage{graphicx} # To include images/logos
  - \usepackage{setspace} # For line spacing
---

```{=tex}
\begin{titlepage}
\begin{center}

\vspace*{2cm} % Adjust vertical spacing as needed

{\Huge \textbf{Car Price in Serbia}} % Project title in bold
\vspace{1cm}

\vfill % Push the rest to the bottom of the page

% Author details
{\large \textbf{Sicheng Huang, Jinmin Li, Zixiang Xiao, Yifan Wang}} \\
University of Toronto

\vspace{1cm}

% Supervisor or professor's details (if applicable)
{\large Prof. So-hee Kang}

\vspace{1cm}

% Submission date
{\large Submitted on: \today}

\end{center}
\end{titlepage}
```
# Task distribution

Sicheng Huang: Cover page; Background and Significance; Impact on the Field; Limitations and Future Research;

Zixiang Xiao: Exploratory Data Analysis; Data Cleaning

Jinmin Li: Model building; Model Selection

Yifan Wang: Diagnostics; Model validation; Key Findings

# Background and significance

Cars are among the most complex mass-produced products, representing the culmination of years of research and development. Which makes pricing of cars a complex process influenced by a wide range of factors that extend beyond simple production costs. It reflects a combination of economic, technological, and market dynamics. As a result, car price reflects a careful balance to ensure profitability, market competitiveness, and consumer accessibility.

The automotive industry in Serbia has a long tradition and plays a significant role in the country’s economy. With factories like Fiat in Kragujevac, Serbia has become a regional hub for the production of cars and components. In addition to major manufacturers, the industry is supported by numerous companies producing parts and providing maintenance and repair services. The market is dynamic, with growing demand for electric and environmentally friendly vehicles, contributing to the modernization and global competitiveness of the sector. Generally speaking, automotive industry in Serbia is significant and makes up about 15% of industry output, which also make automotives aftermarket in Serbia significant.

Based on the data we are given; we have 17 variables (including quantitative variables and dummy variables) that influence after market price in Serbia. Model diagnostics, such as Cook's Distance, are used to identify and address outliers, improving the reliability of the models. Validation is conducted through a training-testing split, revealing that the Interaction Model demonstrates superior predictive performance. The purpose of this research project is to figure out what are the variables that determine car price in Serbia automotives aftermarket.

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Cleaning

```{r,include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
car <- read.csv("serbia_car_sales_price_2024.csv") #load dataset

# change horsepower into numeric values (unit: HP)
car$horsepower_numeric <- as.numeric(gsub(" HP.*", "", car$horsepower))


car$car_brand <- sub(" .*", "", car$car_name)

```

```{r,include=FALSE, message=FALSE, warning=FALSE}
remove_outliers <- function(data, column) {
  Q1 <- quantile(data[[column]], 0.1, na.rm = TRUE)
  Q3 <- quantile(data[[column]], 0.9, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  data %>% filter(data[[column]] >= lower_bound & data[[column]] <= upper_bound)
}

car_data_cleaned_1 <- remove_outliers(car, "price")

# clean the dataset for the first time
#car_data_cleaned= car %>% filter(car_mileage..km < 5*10^5, favorite < 50,
#                                 horsepower_numeric < 400, year >= 1970)

```

```{r,include=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)


car_data_cleaned_1 <- car_data_cleaned_1 %>%
  mutate(
    car_type_1 = case_when( 
      car_type %in% c("limousine") ~ "Luxury",
      car_type %in% c("minivan (MPV)", "caravan") ~ "Family",
      car_type %in% c("pickup") ~ "Utility",
      car_type %in% c("hatchback", "coupe") ~ "Economy",
      car_type %in% c("cabriolet") ~ "Sport",
      car_type %in% c("suv") ~ "SUV",
      TRUE ~ "Other"
    )
  )

car_data_cleaned_1 <-car_data_cleaned_1 %>% mutate(emission_type = case_when(
  emission_class %in% c("Euro 6") ~ "Large Displacement",
   emission_class %in% c("Euro 1","Euro 2","Euro 3","Euro 4","Euro 5") ~ "Small Displacement"
))

car_data_cleaned_1 <- car_data_cleaned_1 %>%
  mutate(
    fuel_type = case_when(
      fuel %in% c("petrol", "diesel", "Diesel + gas", "petrol + gas", "gas") ~ "Oil", 
      fuel %in% c("electric") ~ "Electric", 
      fuel %in% c("hybrid", "methane", "cng", "lpg") ~ "Mixture", 
      TRUE ~ "Unknown"  
    )
  )

car_data_cleaned_1 <- car_data_cleaned_1 %>%
  mutate(
    car_brand_category = case_when(
      
      car_brand %in% c("Audi", "BMW", "Mercedes", "Lexus", "Porsche", "Jaguar", "Land Rover", "Volvo") ~ "Luxury",

      car_brand %in% c("Ford", "Honda", "Hyundai", "Mazda", "Nissan", "Toyota", "Volkswagen", "Skoda", "Mini", 
                       "Opel", "Mitsubishi", "Chevrolet", "Kia", "Subaru", "Seat") ~ "Standard",
     
      car_brand %in% c("Fiat", "Dacia", "Suzuki", "Citroen", "Peugeot", "Renault", "Daewoo", "Daihatsu", 
                       "Lada", "Zastava") ~ "Budget",
      
      car_brand %in% c("Jeep", "Alfa","Dodge", "Chrysler", "Rover", "MG", "Isuzu", "Iveco", "SsangYong", "Smart", 
                       "Saab", "UAZ") ~ "Other",
      
      TRUE ~ "Unknown"
    )
  )




car_data_cleaned_1 <- car_data_cleaned_1[car_data_cleaned_1$emission_class != "", ]

# clean the dataset for the second time
car_data_cleaned_1= car_data_cleaned_1 %>% filter(car_mileage..km < 5*10^5, favorite < 30, 
                                      horsepower_numeric < 300, year >= 2000, fuel != "Unknown", car_brand_category!="Unknown")
```

```{r,include=FALSE,message=FALSE, warning=FALSE}

car_data_cleaned_1$horsepower = as.numeric(gsub(" HP.*", "", car_data_cleaned_1$horsepower))
```

To prepare the car sales dataset for analysis, several data cleaning steps were undertaken to enhance data quality and ensure meaningful insights. Initially, the dataset was loaded, and non-numeric characters in the horsepower column were removed to convert the entries into numeric values. This was achieved by stripping any trailing text such as " HP" and converting the remaining string into a numeric format, facilitating accurate quantitative analysis involving horsepower.

The car_brand was extracted from the car_name by isolating the first word, assuming it represents the brand. This simplification allowed for easier grouping and categorization based on brand attributes. Outliers in the price column were then addressed using a custom function that calculated the 10th and 90th percentiles to establish a robust interquartile range. Data points outside 1.5 times this range were considered outliers and removed, reducing the impact of extreme values on the analysis.

Categorization played a significant role in the data cleaning process. The car_type variable was recoded into broader categories such as "Luxury," "Family," "Utility," and others to simplify analysis and interpretation. Similarly, the emission_class was transformed into an emission_type variable, grouping different emission standards into "Large Displacement" and "Small Displacement" categories. Fuel types were consolidated into categories like "Oil," "Electric," "Mixture," and "Unknown" to streamline fuel-based analyses.

Brands were categorized into "Luxury," "Standard," "Budget," "Other," and "Unknown" segments to enable comparisons across different market tiers. Entries with missing emission classes were removed to maintain data consistency. Further filtering excluded cars with mileage exceeding 500,000 km, horsepower over 300, a "favorite" count over 30, and manufacturing years before 2000. Additionally, entries with "Unknown" fuel types or brand categories were omitted to focus on data with clear, usable information.

Finally, the horsepower column was reconverted to numeric to ensure consistency after the transformations and filtering. These comprehensive data cleaning steps resulted in a refined dataset, free from anomalies and inconsistencies, thereby facilitating a more accurate and insightful analysis of the car sales trends in Serbia for 2024.

# Exploratory Data Analysis

To begin our investigation, we examined our dataset of car sales, which consists of 6,190 observations after cleaning. The continuous variables under consideration are price, views, favorite count, car mileage (in kilometers), engine capacity (in cubic centimeters), year of manufacture, horsepower, and number of seats. By plotting violin charts for each variable's distribution, we observed that most variables are moderately distributed, with price and horsepower showing noticeable skewness toward the lower end. Specifically, car mileage tends to be skewed toward higher values, indicating that many cars have accumulated significant mileage.

Further examination using violin plots of price against categorical variables such as fuel type, air conditioning type, emission type, car type, car brand category, gearbox type, color, and number of doors revealed distinct patterns. For instance, cars classified under the "Luxury" brand category tend to have higher prices compared to those in the "Standard" or "Budget" categories. Similarly, vehicles with automatic gearboxes generally command higher prices than those with manual transmissions.

```{r,include=FALSE,message=FALSE, warning=FALSE}
 ggplot(car_data_cleaned_1, aes(x = 1, y = price)) +
  geom_violin(fill = "blue", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Price",
    x = "",
    y = "Price"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = views)) +
  geom_violin(fill = "green", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Views",
    x = "",
    y = "Views"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )

```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = favorite)) +
  geom_violin(fill = "purple", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Favorite",
    x = "",
    y = "Favorite"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = car_mileage..km)) + 
  geom_violin(fill = "orange", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Car Mileage (km)",
    x = "",
    y = "car_mileage, km"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )

```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = engine_capacity..cc)) + 
  geom_violin(fill = "red", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Engine Capacity (cc)",
    x = "",
    y = "Engine Capacity (cc)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )

```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = year)) +
  geom_violin(fill = "cyan", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Year",
    x = "",
    y = "Year"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = horsepower)) +
  geom_violin(fill = "pink", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Horsepower",
    x = "",
    y = "Horsepower"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )

```

```{r,include=FALSE,message=FALSE, warning=FALSE}
ggplot(car_data_cleaned_1, aes(x = 1, y = seats_amount)) +
  geom_violin(fill = "yellow", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(
    title = "Violin Plot of Seats Amount",
    x = "",
    y = "Seats Amount"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

```{r,echo=FALSE,message=FALSE, warning=FALSE,fig.width=6, fig.height=3}
library(ggplot2)
library(gridExtra)

# Create individual plots
p1 <- ggplot(car_data_cleaned_1, aes(x = 1, y = price)) +
  geom_violin(fill = "blue", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Price") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

p2 <- ggplot(car_data_cleaned_1, aes(x = 1, y = views)) +
  geom_violin(fill = "green", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Views") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

p3 <- ggplot(car_data_cleaned_1, aes(x = 1, y = favorite)) +
  geom_violin(fill = "purple", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Favorite") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

p4 <- ggplot(car_data_cleaned_1, aes(x = 1, y = car_mileage..km)) + 
  geom_violin(fill = "orange", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "car_mileage, km") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p5 <- ggplot(car_data_cleaned_1, aes(x = 1, y = engine_capacity..cc)) + 
  geom_violin(fill = "red", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "engine_capacity(cc)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p6 <- ggplot(car_data_cleaned_1, aes(x = 1, y = year)) +
  geom_violin(fill = "cyan", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Year") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

p7 <- ggplot(car_data_cleaned_1, aes(x = 1, y = horsepower)) +
  geom_violin(fill = "pink", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Horsepower") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

p8 <- ggplot(car_data_cleaned_1, aes(x = 1, y = seats_amount)) +
  geom_violin(fill = "yellow", alpha = 0.5, trim = FALSE) +
  geom_boxplot(width = 0.1, alpha = 0.7) +
  labs(title = "", x = "", y = "Seats Amount") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), plot.title = element_text(hjust = 0.5))

# Combine all 8 plots into a 2-row, 4-column grid
grid.arrange(p1, p2, p3, p4,p5, p6, p7, p8, ncol = 4)

```

```{r,echo = FALSE,message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
library(ggplot2)
library(gridExtra)

# Create individual plots
p1 <- ggplot(car_data_cleaned_1, aes(x =fuel_type , y = price, fill = fuel_type)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "Fuel Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none") +theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p2 <- ggplot(car_data_cleaned_1, aes(x = `A.C`, y = price, fill = `A.C`)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "A/C Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none") +theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p3 <- ggplot(car_data_cleaned_1, aes(x = emission_type, y = price, fill = emission_type)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "emission class Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p4 <- ggplot(car_data_cleaned_1, aes(x = car_type_1, y = price, fill = car_type_1)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "type of car",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p5 <- ggplot(car_data_cleaned_1, aes(x = car_brand_category, y = price, fill = car_brand_category)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "type of drive Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p6 <- ggplot(car_data_cleaned_1, aes(x = gearbox, y = price, fill = gearbox)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "type of gearbox Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p7 <- ggplot(car_data_cleaned_1, aes(x = color, y = price, fill = color)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "type of color Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

p8 <- ggplot(car_data_cleaned_1, aes(x = doors, y = price, fill = doors)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA, fill = "white") +
  labs(title = "",
       x = "Doors Type",
       y = "Price (Euro)") +
  theme_minimal() +
  theme(legend.position = "none")+theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))

# Combine the 8 plots using grid.arrange
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4)

```

We conducted a multiple linear regression analysis to model the car prices based on these variables. The summary of the regression model indicates that several predictors are statistically significant. Notably, year of manufacture and horsepower have strong positive relationships with price, suggesting that newer cars and those with more power are valued higher in the market. The emission type also plays a significant role; cars with "Small Displacement" engines tend to have lower prices, possibly due to perceptions about performance or efficiency.

Interestingly, the favorite count shows a significant negative relationship with price. This could imply that more affordable cars receive more interest or that users are favoriting cars they aspire to purchase. Car mileage negatively impacts price as expected, with higher mileage reducing the vehicle's value. The type of drive (e.g., front-wheel drive) and certain fuel types like "Oil" and "Mixture" also negatively affect the price.

```{r, include= FALSE,message=FALSE, warning=FALSE}
full_model <- lm(price ~ favorite + post_info + year + A.C + emission_type + seats_amount + horsepower_numeric  + car_mileage..km + type_of_drive + doors + fuel_type  + gearbox + color + car_type_1 + car_brand_category , data = car_data_cleaned_1)


summary(full_model)

library(MASS)
full_model_step <- step(full_model, direction = "backward")
summary(full_model_step)

```

```{r,echo=FALSE,message=FALSE, warning=FALSE, fig.width=5, fig.height=4}
library(car)
numeric_only = lm(formula = price ~  seats_amount + horsepower_numeric + 
    year + car_mileage..km +     favorite , data = car_data_cleaned_1)

avPlots(numeric_only)

```

Variables such as seats amount, specific car types (like SUVs and Sports cars), and belonging to a "Luxury" or "Standard" brand category positively influence the price. The adjusted R-squared value of 0.7922 indicates that approximately 79% of the variability in car prices is explained by the model, signifying a strong fit.

In summary, the analysis reveals that factors such as year of manufacture, horsepower, emission type, car mileage, car type, and brand category are significant predictors of car price in our dataset. These findings can help buyers and sellers understand the key attributes that influence car valuation in the Serbian market for 2024.

```{r, echo=FALSE,message=FALSE, warning=FALSE, fig.width=5, fig.height=4}

#define numeric columns
numeric_cols <- c("year","car_mileage..km","engine_capacity..cc",
                  "horsepower_numeric","seats_amount","views","favorite")

library(corrplot)

#compute correlation matrix
cor_matrix <- cor(car[,numeric_cols], use = "complete.obs", method = "pearson")

#create correlation matrix plot
corrplot(cor_matrix, method = "number", type = "lower",
         diag = FALSE,  tl.col="black", tl.srt=45)
```

After gaining insights on the relationships between car prices and individual factors, we can now examine the correlation among all the factors in the dataset using the correlation matrix shown in the heatmap above. The heatmap visualizes the relationships between the continuous variables, including year, car_mileage(km), engine_capacity(cc), horsepower_numeric, seats_amount, views, and favorite.

It is evident that views and favorite have a relatively strong correlation (0.76), indicating that cars with more views tend to have more favorites as well. This relationship suggests that these two variables are highly related, which may lead to issues like multicollinearity during model training. Therefore, it could be beneficial to consider dropping one of these factors when building a predictive model to avoid redundancy.

Another noticeable correlation is between engine_capacity..cc and horsepower_numeric (0.63), which makes intuitive sense, as larger engine capacities are typically associated with higher horsepower. However, retaining both these variables might result in moderate multicollinearity. Similarly, year shows a positive correlation with horsepower_numeric (0.26), indicating that newer car models tend to have more powerful engines.

```{r, echo=FALSE,message=FALSE, warning=FALSE}
library(car)
lm_model <- lm(price ~ year + car_mileage..km + engine_capacity..cc + horsepower_numeric + 
                 seats_amount + views + favorite, data = car_data_cleaned_1)
vif_values <- vif(lm_model)

print(vif_values)

```

The VIF analysis for the variables indicates that none of the predictors exhibit problematic multicollinearity, as all VIF values are below 5. The highest VIF values are for favorite (1.663) and views (1.649), suggesting some moderate correlation but not enough to warrant immediate exclusion. Given that all values are relatively low, we can conclude that multicollinearity is not a major concern in this model, and all predictors can be retained for further analysis without significantly affecting model stability or interpretability.

# Model

## Overview of the Models

In this section, we developed three models to improve the accuracy of our predictions progressively: the main effect model, the interaction model, and the final model. Throughout this process, we added variables we deemed essential and removed those with minimal impact on the model to enhance its predictive performance.

## Main Effect Model

We want to build a main effects model. After preliminary cleaning the data, we address multicollinearity by selecting one variable from each pair of variables highly correlated with one another; here we retain “favorite” and “horsepower.” After making a full model with all the variables, we have back-stepped out those whose t-value is small: “color”. This completes our main effects model.

```{r include=FALSE}
# build up overall rough model
full_model <- lm(price ~ favorite + factor(post_info) + year + factor(A.C) + factor(emission_type) + seats_amount + horsepower_numeric + car_mileage..km + factor(type_of_drive) + factor(doors) + factor(fuel_type) + factor(gearbox) + factor(color) + factor(car_type_1) + factor(car_brand_category), data = car_data_cleaned_1)
```

-   We used the step function to perform backward stepwise selection on the full model, iteratively removing variables with minimal impact to derive the initial Main Effect Model.

```{r include=FALSE}
library(MASS)
Main_Effect_Model <- step(full_model, direction = "backward")
```

```{r echo=FALSE}
Main_Effect_Model <- lm(formula = price ~ favorite +  year + factor(A.C) + factor(emission_type) + seats_amount + horsepower_numeric + car_mileage..km + factor(type_of_drive) + factor(doors) + factor(fuel_type) + factor(gearbox) + factor(car_type_1) + factor(car_brand_category), data = car_data_cleaned_1)

temp = step(Main_Effect_Model, direction = "backward")
```

-   Based on the results of the AIC method, we identified a simplified model with the following structure and summary, which we define as the Main Effect Model.We removed the variable "color" based on the AIC, and we excluded "post_info" due to its lack of statistical significance. Subsequently, we evaluated the final Main Effect Model by examining its coefficients, $R^2$, and adjusted $R^2$ , $\text{Adjusted } R^2$. We observed a reduction in $\text{Adjusted } R^2$, likely due to the decrease in the number of variables included in the model.This leads us to think of building the Interaction Model.

-   In the Main Effect Model, the Multiple $R^2$ value is 0.7938, indicating that approximately 79.38% of the variability in the dependent variable is explained by the model. The $\text{Adjusted } R^2$ is 0.7922, which accounts for the number of predictors in the model and suggests a strong overall fit.

-   Formula of Main Effect Model: \begin{align*}
    \text{price} = \beta_0 \; &+ \beta_1 \cdot \text{factor(emission\_type)} + \beta_2 \cdot \text{seats\_amount} + \beta_3 \cdot \text{horsepower\_numeric} + \beta_4 \cdot \text{year} \\
    &+ \beta_5 \cdot \text{factor(car\_type\_1)} + \beta_6 \cdot \text{factor(fuel\_type)} + \beta_7 \cdot \text{factor(gearbox)} + \beta_8 \cdot \text{factor(A.C)} \\
    &+ \beta_9 \cdot \text{car\_mileage\_km} + \beta_{10} \cdot \text{favorite} + \beta_{11} \cdot \text{factor(car\_brand\_category)} +  \epsilon
    \end{align*}

```{r include=FALSE}
summary.main_effect_model = summary(Main_Effect_Model)
```

## Interaction model

From the car price data, we suspected that factors such as mileage, horsepower, and seating capacity may interact in a way that the combined effect on price is not simple. We noted that these continuous variables can interact with other predictors, such as the age or the number of seats, in order to improve the predictive power of our model.

-   car_mileage..km \* year : High mileage decreases the price of newer cars more significantly than older ones. When a newer car has high mileage, it indicates that it has been driven extensively in a short period, which may treat as a red flag for buyers.

-   horsepower_numeric \* year :Technological improvements in rencent years allow newer cars to have higher horsepower without drawbacks, altering its price impact.

```{r echo=FALSE}
interaction_model_1 <- lm(formula = price ~ factor(emission_type)  + horsepower_numeric  + year +
                         factor(car_type_1) + factor(fuel_type) + factor(gearbox) + factor(A.C) + car_mileage..km + favorite + factor(car_brand_category) +
                         car_mileage..km * year + horsepower_numeric * year,
                         data = car_data_cleaned_1)
full_model_2 <- lm(price ~ favorite + factor(post_info) + year + factor(A.C) + factor(emission_type) + seats_amount + horsepower_numeric + car_mileage..km + factor(type_of_drive) + factor(doors) + factor(fuel_type) + factor(gearbox) + factor(color) + factor(car_type_1) + factor(car_brand_category)+
                         car_mileage..km * year + horsepower_numeric * year, data = car_data_cleaned_1)

```

-   Backward stepwise selection systematically removes variables included that contribute minimally towards explanatory power using as criteria the AIC.

```{r echo=FALSE}
interaction_model <- step(interaction_model_1, direction = "backward")
summary.interaction_model = summary(interaction_model)
```

-   Base on AIC, the step function gives us a more accurate variable list, by removing "seats_amount" and doors. We finalize the summing up of the model, retaining more impactful variables, and now present improved adjusted coefficients and newly achieved adjusted R-squared, reflecting our further enhanced predictive performance:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Install and load gt
#install.packages("gt")
library(gt)
library(broom)

# Create a styled table
coef_table <- tidy(interaction_model)
coef_table <- coef_table 
gt(coef_table)

```

-   We found that through the interaction terms we increased the $\text{Adjusted } R^2$ from 79.83% to 83.16%. Since that had been done, we could use it to our advantage and help create a better model. Now, we think such inclusion of these interaction terms tremendously enhanced the predictive power of our model, and had been the correct step in our analysis.

-   year:car_mileage..km (-0.000679): As the interaction between year and car_mileage..km increases, the predicted price decreases slightly. A one-unit increase in their product reduces price by 0.000679.

-   horsepower_numeric:year (5.03): A positive interaction—higher horsepower and later years jointly increase the price. Each unit increase in their product raises price by 5.03.

-   We can conclude our interaction model with this formula: \begin{align*}
    \text{price} = \beta_0 \; &+ \beta_1 \cdot \text{factor(emission\_type)} + \beta_2 \cdot \text{seats\_amount} + \beta_3 \cdot \text{horsepower\_numeric} + \beta_4 \cdot \text{year} \\
    &+ \beta_5 \cdot \text{factor(car\_type\_1)} + \beta_6 \cdot \text{factor(fuel\_type)} + \beta_7 \cdot \text{factor(gearbox)} + \beta_8 \cdot \text{factor(A.C)} \\
    &+ \beta_9 \cdot \text{car\_mileage\_km} + \beta_{10} \cdot \text{favorite} + \beta_{11} \cdot \text{factor(car\_brand\_category)} \\
    &+ \beta_{12} \cdot (\text{car\_mileage\_km} \times \text{year}) + \beta_{13} \cdot (\text{horsepower\_numeric} \times \text{year}) + \epsilon
    \end{align*}

-   After this step, we was on going into the testing of the assumption of linearity. When we delve a little deeper into the diagnostic tests, we find that residual analysis and investigation of multicollinearity between predictors. So we added the power and logarithmic transformation to the variable, to help solve the problem.

## Final model (with transformation)

After the first two steps, we realized that we needed to perform some transformations on the variables to achieve a suitable linear model. So, we proceeded to the third step, where we applied logarithmic and square root transformations to certain variables.

-   **favorite**: almost all the points were clustered on the far right side of the scatter plot. This indicated a high degree of skewness in its distribution. To address this, we applied a transformation to make its distribution more uniform, which improved the accuracy of our predictions.

-   We also found that the **car_mileage** variable had values that varied greatly—some cars had only been driven fewer than one hundred kilometers, while others had been driven around 300,000 kilometers. Even though we removed some extreme values during data cleaning, the range was still quite large. To concentrate the data and reduce the impact of outliers, we applied a square root transformation to this variable.

-   We can represent our regression model as: \begin{align*}
    \text{price} = \beta_0 \; &+ \beta_1 \cdot \text{factor(emission\_type)} + \beta_2 \cdot \text{horsepower\_numeric} + \beta_3 \cdot \text{year} \\
    &+ \beta_4 \cdot \text{factor(car\_type\_1)} + \beta_5 \cdot \text{factor(fuel\_type)} + \beta_6 \cdot \text{factor(gearbox)} \\
    &+ \beta_7 \cdot \sqrt{\text{car\_mileage..km}} + \beta_8 \cdot \text{factor(A.C)} \\
    &+ \beta_9 \cdot (\text{car\_mileage..km} \times \text{year}) + \beta_{10} \cdot (\text{horsepower\_numeric} \times \text{year}) \\
    &+ \beta_{11} \cdot \log(\text{favorite} + 1) + \beta_{12} \cdot \text{car\_brand\_category} \\
    &+ \epsilon
    \end{align*}

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Center the variables
# car_data_cleaned_1$horsepower_numeric_centered <- car_data_cleaned_1$horsepower_numeric - mean(car_data_cleaned_1$horsepower_numeric)
# car_data_cleaned_1$car_mileage_centered <- car_data_cleaned_1$car_mileage..km - mean(car_data_cleaned_1$car_mileage..km)
# car_data_cleaned_1$year_centered <- car_data_cleaned_1$year - mean(car_data_cleaned_1$year)


modified_final_model <- lm(
  formula = price ~ factor(emission_type)  + horsepower_numeric + year +
    car_type_1 + fuel_type + gearbox + I((car_mileage..km)^(1/2)) + factor(A.C) +
    car_mileage..km * year + horsepower_numeric * year +
    log(favorite + 1) + car_brand_category,
  data = car_data_cleaned_1
)

full_model_3 <- lm(price ~ factor(post_info) + year + factor(A.C) + factor(emission_type) + seats_amount + horsepower_numeric + factor(type_of_drive) + factor(doors) + factor(fuel_type) + factor(gearbox) + factor(color) + factor(car_type_1) + factor(car_brand_category)+
                         car_mileage..km * year + horsepower_numeric * year + + I((car_mileage..km)^(1/2)) + log(favorite + 1), data = car_data_cleaned_1)


```

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# modified_1127<- lm(formula = price ~  factor(emission_type) + seats_amount + horsepower_numeric + year +car_type_1 + fuel_type + gearbox+ I((car_mileage..km)^(1/2)) + factor(A.C)  + car_mileage..km * year +
#                  horsepower_numeric * year + horsepower_numeric * seats_amount +
#                   log(favorite + 1) + car_brand_category , data = car_data_cleaned_1)

final_model <- stepAIC(modified_final_model, direction = "backward")
summary.final_model = summary(final_model)
```

-   Base on AIC, the step function gives us a more accurate variable list, we don't need to subtract variables at this stage.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Install and load gt
#install.packages("gt")
library(gt)
library(broom)

# Create a styled table
coef_table <- tidy(final_model)
coef_table <- coef_table 
gt(coef_table)

```

-   Interpretation of transformation terms: For a 1-unit increase in log(favorite + 1), the price decreases by approximately 307 units. This suggests that higher favorite values are associated with lower prices.

-   The coefficient of I((car_mileage..km) \^1/2 tells us 3.58 the car price is expected to change for a 1-unit increase in the square root of mileage, holding other variables constant. However, this is counterintuitive, as higher mileage often reduces value. This could be due to correlations with other variables in the model, and we need further investigations.

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(qpcR)
library(olsrr)
library(knitr)
library(gt)

r_squared_main <- summary.main_effect_model$r.squared
adj_r_squared_main <- summary.main_effect_model$adj.r.squared

r_squared_interaction <- summary.interaction_model$r.squared
adj_r_squared_interaction <- summary.interaction_model$adj.r.squared

r_squared_final <- summary.final_model$r.squared
adj_r_squared_final <- summary.final_model$adj.r.squared


bic_main <- BIC(Main_Effect_Model)
bic_interaction <- BIC(interaction_model)
bic_final <- BIC(final_model)

aic_main <- 93206.13
aic_interaction <- 91915.41
aic_final <- 91862.12

cp_main <- ols_mallows_cp(Main_Effect_Model, full_model)
cp_interaction <- ols_mallows_cp(interaction_model, full_model_2)
cp_final <- ols_mallows_cp(final_model, full_model_3)

#build up dataframe
model_comparison <- data.frame(
  Model = c("Main Effect Model", "Interaction Model", "Final Model"),
  R_Squared = c(r_squared_main, r_squared_interaction, r_squared_final),
  Adjusted_R_Squared = c(adj_r_squared_main, adj_r_squared_interaction, adj_r_squared_final),
  AIC = c(aic_main, aic_interaction, aic_final),
  BIC = c(bic_main, bic_interaction, bic_final),
  Cp = c(cp_main, cp_interaction, cp_final)
)

kable(model_comparison, caption = "Comparison of Models")

```

# Model Selection

-   Model Fit:

    -   The $R^2$ and Adjusted $R^2$ values increase progressively from the Main Effect Model ($R^2 = 0.791$, Adjusted $R^2 = 0.790$) to the Final Model ($R^2 = 0.832$, $Adjusted R^2 = 0.831$). This indicates that each successive model explains more variation in the data.

-   Model Complexity:

    -   The $C_p$ values increase across the models, from 84.58 in the Main Effect Model to 143.25 in the Final Model, suggesting that the Final Model is more complex, with additional parameters included.

-   Model Selection Criteria:

    -   The AIC and BIC values decrease slightly from the Main Effect Model (AIC = 93206.13, BIC = 110963.0) to the Final Model (AIC = 91862.12, BIC = 109612.3). The lowest values in the Final Model indicate that it is the most efficient model in balancing goodness of fit and complexity.

-   Final Assessment: While the final model sustains its marginal improvement on both terms of $R^2$ and $Adjusted R^2$, along with improved but substantially greater complexity with ($C_p = 143.25$). Also, with somewhat superior AIC and BIC values for better performance to stand in the most of these above criteria, it is the final model with the superiority mark for the excellence in preforming it.

# Diagnostics

-   After cleaning the data we run diagnostics on the model to check for model assumptions.

-   The following graphs would give us the best feedback on the model assumptions:

```{r echo=FALSE,}
par(mfrow = c(2, 2)) # Set up a 2x2 plotting layout
plot(final_model) # Automatically generates the 4 diagnostic plots
```

-   From the first graph we can know the residuals are roughly centered around zero across the fitted values, which supports normality in terms of mean zero.

-   Points align closely with the diagonal line in the central portion of the plot, which supports normality for most residuals in the Q-Q plot.

-   According to the third graph, residual variance is almost constant, but the spread does not seem extreme enough to strongly violate normality.

-   From the last graph, the points may disproportionately impact the residuals but do not necessarily negate normality for the majority of data points.

-   In conclusion, the residuals demonstrate reasonable normality across most of their distribution, especially in the central range. While there are deviations in the tails, the model generally satisfies the assumption of normality for residuals.

# Model Validation

```{r include=FALSE}
# library(caret)
data <- car_data_cleaned_1
# 
# categorical_vars <- c('emission_class', 'seats_amount', 'car_type', 'fuel', 'gearbox',
#                       'type_of_drive', 'post_info', 'A.C', 'doors')
# data[categorical_vars] <- lapply(data[categorical_vars], as.factor)
# 
# data <- data[complete.cases(data), ]

# Center the variables
data$horsepower_numeric <- car_data_cleaned_1$horsepower_numeric - mean(car_data_cleaned_1$horsepower_numeric)
data$car_mileage <- car_data_cleaned_1$car_mileage..km - mean(car_data_cleaned_1$car_mileage..km)
data$year <- car_data_cleaned_1$year - mean(car_data_cleaned_1$year)

```

```{r echo=FALSE,message=FALSE, warning=FALSE, fig.width=5, fig.height=3}

set.seed(11)
k <- 30

data <- data[sample(1:nrow(data)), ]

folds <- cut(seq(1, nrow(data)), breaks = k, labels = FALSE)

mse_list <- c()
mspe_list <- c()

for (i in 1:k) {
  validation_indices <- which(folds == i, arr.ind = TRUE)
  validation_data <- data[validation_indices, ]
  training_data <- data[-validation_indices, ]
  
  model <- lm(formula = final_model, data = training_data)
  
  train_predictions <- predict(model, newdata = training_data)
  validation_predictions <- predict(model, newdata = validation_data)
  
  train_residuals <- training_data$price - train_predictions
  validation_residuals <- validation_data$price - validation_predictions
  
  mse <- mean(train_residuals^2)
  mspe <- mean(validation_residuals^2)
  
  mse_list <- c(mse_list, mse)
  mspe_list <- c(mspe_list, mspe)
}

average_mse <- mean(mse_list)
average_mspe <- mean(mspe_list)

results_table <- data.frame(
  Metric = c("Average MSE (Training)", "Average MSPE (Validation)"),
  Value = c(average_mse, average_mspe)
)

print(results_table)

par(mfrow = c(1, 2))
plot(train_residuals, main = "Training Residuals")
plot(validation_residuals, main = "Validation Residuals")

```

-   During cross-validation, we used 70% of the data to train the model, and 30% of the data to keep aside for validation to ensure that the model is evaluated on data it has not seen during training. This approach helps to evaluate the model's performance more robustly and ensures that it is not overfitting to the training data. The Average MSE (Training) for the training dataset is 2762565 which tells us the average squared difference between the predicted and actual values on the training data. Average MSPE (Validation): The Average MSPE (Validation) for the validation dataset is 2820394. The slight difference between the MSE and MSPE shows that the model is performing similarly on both the training and validation datasets, which is a good sign of its generalizability.

```{r eval=FALSE,echo=FALSE}
# simple train-validation split
train_index <- createDataPartition(data$price, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
validation_data <- data[-train_index, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Validation set size:", nrow(validation_data), "\n")

final_model <- lm(formula = final_model, data = train_data)

train_predictions <- predict(final_model, newdata = train_data)

train_residuals <- train_data$price - train_predictions

validation_predictions <- predict(final_model, newdata = validation_data)

validation_residuals <- validation_data$price - validation_predictions

validation_predictions <- predict(final_model, newdata = validation_data)

validation_residuals <- validation_data$price - validation_predictions

MSE_F <- mean(train_residuals^2)
MSPR <- mean(validation_residuals^2)

results_table <- data.frame(
  Metric = c("MSE_F (Training MSE)", "MSPR (Validation MSE)"),
  Value = c(MSE_F, MSPR)
)


print(results_table)


```

# Key Findings

-   Our data analysis reveals a complex relationship between various car features and prices, with significant interactions between certain variables. It suggests that both direct factors like horsepower and indirect factors like car mileage influence the price and that these effects are moderated by other characteristics such as the car's age or size. We reduced some variables to decrease complexity. For example, we optimize the number of car_type categories from 8 to 3 and reduce fuel type categories. Price data was simplified to focus on the 10th to 90th quantiles, we also removed extreme outliers like cars with more than 500,000 km mileage or cars with more than 300 horsepower.

-   According to the study, we found strong correlations between variables like views and favorites, and between engine_capacity and horsepower. This problem was solved by selecting the most significant variables and we selected horsepower numeric and removed engine capacity.

-   The main effect model, interaction model, and transformation and power model were used to improve model performance which shows the complexity of the relationships affecting pricing. Firstly, the main Effect Model focused on the relationships between features and car price. Secondly, the interaction model interacted with terms such as horsepower numeric \* year car_mileage \* year and horsepower numeric \* seat amount to find complex relationships. Thirdly, transformation and power model: logarithmic transformations of variables like favorite and square root transformations for car mileage.

-   These analyses revealed key variables such as emission_type, seat amount horsepower numeric, year car type 1, fuel type, gearbox, A.C, and car brand category are significantly associated with pricing so they can be key predictors. Additionally, the horsepower and the car mileage have relationships with the year by the interaction model. However, the year also emerges as a significant predictor of price when analyzed independently. In conclusion, if we want to buy a car with high horsepower, low car mileage, and fewer years, we need to pay more money.

# Impact on the Field

In conclusion, we developed a model with a respectable adjusted R\^2 value of 0.8331, indicating that the model explains 83.31% of the variation in the dataset. By refining the dataset, grouping similar categories, and incorporating interaction and polynomial terms, we significantly reduced the number of parameters from 95 to 66 during data cleaning and further to 28 in the final model. Our model provides a solution for real life scenarios for customers to evaluate car price using simplify and optimized model.

Our analysis revealed that the most influential positive factors on car prices are ‘Car Type (Sport)’, ‘Car Brand Category (Luxury)’, and ‘Year’, while the most influential negative factors are ‘Horsepower Numeric’, ‘Fuel Type (Oil)’, and ‘Gearbox Manual (6 Speeds)’. These insights highlight the critical variables driving car pricing and underscore the model's efficiency in capturing key relationships within the data.

# Limitations and Future Research

Our current model has significant limitations that require attention for improvement. First, the under representation of certain car types—electric cars (12 instances), hybrid cars (39), gas-powered cars (6), and diesel + gas cars (6)—introduces challenges such as data imbalance, model instability, and issues with statistical significance. This imbalance makes it difficult for the model to accurately generalize across all car types. Additionally, our QQ plot reveals deviations at both tails, indicating the presence of outliers that likely skew the results and reduce model accuracy.

Moreover, the high Mean Squared Error (MSE) difference suggests potential overfitting, which may stem from excessive model complexity or the influence of outliers. Another critical limitation lies in the categorization process we employed to reduce the number of parameters. While simplification was necessary, it inherently introduces subjectivity, potentially leading to inaccuracies in real-world predictions.

To improve the model, we need more data, especially for underrepresented car types like electric, hybrid, gas-powered, and diesel + gas cars, to address data imbalance and reduce bias. Techniques like oversampling rare types, weighting loss functions, and robust methods for handling outliers can further enhance performance. Simplifying the model with regularization can prevent overfitting, while objective categorization methods, such as clustering or expert input, can reduce subjectivity. Adding external variables like fuel availability or economic conditions could also improve predictions. These steps will make the model more balanced, accurate, and practical.

# Reference 

Jelenković, S., Brzaković, A., & Mihailović, B. (2020). The role and importance of dealers (sellers) for the automobile market in Serbia. Oditor, 6(3), 7–32. https://doi.org/10.5937/oditor2003007j 

Domazet, I., & Stosic, I. (2017). Basic characteristics of competitive relations in the after-sales market of motor vehicles in Serbia. Ekonomika Preduzeca, 65(5–6), 413–426. https://doi.org/10.5937/ekopre1706413d 
